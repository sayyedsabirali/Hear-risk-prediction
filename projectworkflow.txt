1. Project Setup & Local Environment

1. Create project template by executing template.py.
2. Write code in setup.py and pyproject.toml to import local packages from src.
3. Add -e . in requirements.txt to ensure local packages can be accessed outside venv without conflicts.
4. Create a virtual environment and activate it:
   python -m venv heart-risk
   heart-risk\Scripts\activate
   Add required packages to requirements.txt (including mlflow, dagshub, dvc, awscli, flask, kubernetes client, prometheus_client) and install:
   pip install -r requirements.txt
   Do pip list to ensure all packages are installed, including local packages.

2. MongoDB Setup

1. Sign up for MongoDB Atlas, create a new project and a cluster (M0 free tier).
2. Setup DB username & password.
3. Add network access: 0.0.0.0/0 (for global access).
   - Copy MongoDB connection string (Python driver) and store in environment variable:
   - MONGODB_URL="mongodb+srv://<username>:<password>...."
   - Create notebook folder, add dataset, push to MongoDB from a notebook, verify data in MongoDB Atlas.


3. Logging & Exception Handling

1. Create a logger.py file, test it on demo.py.
2. Create an exception.py file, test it on demo.py.


4. Data Ingestion Component

1. Define variables in constants/__init__.py.
2. Add MongoDB connection logic in configuration/mongo_db_connections.py.
3. Create data_access/proj1_data.py to fetch data from MongoDB, transform key-value format to DataFrame.
4. Add DataIngestion configs and artifact classes:
   entity/config_entity.py → DataIngestionConfig
   entity/artifact_entity.py → DataIngestionArtifact
   Implement components/data_ingestion.py and integrate it into training_pipeline.Test full ingestion by running demo.py (ensure MONGODB_URL env variable is set).


5. Data Validation & Transformation

1. Define dataset schema in config/schema.yaml.
2. Add helper functions in utils/main_utils.py.
3. Implement DataValidation component like ingestion (configs, artifacts, components).


6  Implement DataTransformation component:

1. Add estimator.py in entity/ folder for transformers/scalers.
2. Implement ModelTrainer component:
3. Add model training logic and estimator class in estimator.py.


7. MLFlow / Dagshub Experiment Tracking

1.Connect project to Dagshub repository.
2. Install and configure MLFlow (pip install mlflow dagshub) for experiment tracking.
3. Set up tracking URI in constants/__init__.py and integrate into training pipeline.
4. Run experiments and log metrics, parameters, models.
5. Commit & push notebooks with experiments.


8. DVC Setup & S3 Remote Storage

1. Initialize DVC in project: dvc init
2. git add . commit 
3. dvc remote add -d myremote s3://sabir-project/dvcstore
2. Create temporary local folder local_s3.
3. Configure DVC remote: dvc remote add -d myremote s3://<bucket-name>
4. Configure AWS credentials in environment variables or constants/__init__.py:
5. AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION
6. Push data/artifacts to S3 using DVC: dvc push
7. Track pipeline stages using dvc.yaml and parameters in params.yaml.


9. Model Evaluation & Pusher

1. Define evaluation thresholds and S3 keys in constants/__init__.py:
2. MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE = 0.02
3. MODEL_BUCKET_NAME = "my-model-mlopsproj"
4. MODEL_PUSHER_S3_KEY = "model-registry"
Implement ModelEvaluation and ModelPusher components.
Push best model to S3 bucket for deployment.



10. Prediction Pipeline & Fastapi App

1. Create prediction_pipeline structure.
2. Setup app.py with REST endpoints for /predict and /train.
3. Add static and template directories for frontend.
Test locally using Fastapi: python app.py

11. CI/CD Pipeline

Getting started with CI-CD process:

      * Setup the dockerfile and .dockerignore file
      * Setup the .github\workflows dir and aws.yaml file within
      * Go to AWS console and create a new IAM user exactly the way we did earlier (name: "usvisa-user") >>
        Go inside user >> Security Credentials >> Access Keys >> create access key >> CLI >> check agreement
        >> next >> create access key >> download csv
      * Now create one ECR repo to store/save docker image:
        AWS console >> Go to ECR >> Region: us-east-1 >> Hit create repository >>
        repo name: vehicleproj >> hit create repository >> copy and keep uri
      * Now create EC2 Ubuntu server >> AWS console >> EC2 >> Launch Instance >> name: vehicledata-machine
        >> Image: Ubuntu >> AMI: Ubuntu Server 24.04 (free tier) >> Instance: T2 Medium (~chargeable-3.5rs/hr)
        >> create new key pair (name: proj1key) >> allow for https and http traffic >> storage: 30gb >> Launch
        >> Go to instance >> click on "Connect" >> Connect using EC2 Instance Connect 
        >> Connect (Terminal will be launched)

Open EC2 and Install docker in EC2 Machine:

      ## Optinal
      sudo apt-get update -y
      sudo apt-get upgrade
      ## Required (Because Docker is'nt there in our EC2 server - [docker --version])
      curl -fsSL https://get.docker.com -o get-docker.sh
      sudo sh get-docker.sh
      sudo usermod -aG docker ubuntu
      newgrp docker

Next step is to connect Github with EC2(Self hosted runner):
      * select your project on Github >> go to settings >> Actions >> Runner >> New self hosted runner
        >> Select OS (Linux) >> Now step by step run all "Download" related commands on EC2 server 
        >> run first "Configure" command (hit enter instead of setting a runner group, runner name: self-hosted)
        >> enter any additional label (hit enter to skip) >> name of work folder (again hit enter)
        >> Now run second "Configure" command (./run.sh) and runner will get connected to Github
        >> To crosscheck, go back to Github and click on Runner and you will see runner state as "idle"
        >> If you do ctrl+c on EC2 server then runner will shut then restart with "./run.sh"

Setup your Github secrets: (Github project>Settings>SecretandVariable>Actions>NewRepoSecret)
      AWS_ACCESS_KEY_ID
      AWS_SECRET_ACCESS_KEY
      AWS_DEFAULT_REGION
      ECR_REPO

CI-CD pipeline will be triggered at next commit and push.

1️⃣1️⃣ EC2 Deployment

Launch Ubuntu EC2 instance, install Docker.

Run container on EC2:
docker run -p 5000:5000 project-app:latest

Allow inbound traffic for port 5000 via Security Group.

Access app via public IP: http://<ec2-ip>:5000

1️⃣2️⃣ Kubernetes (EKS) Deployment

Install kubectl and eksctl.

Create EKS cluster:

eksctl create cluster --name project-cluster --region us-east-1 --nodegroup-name project-nodes --node-type t3.small --nodes 1 --nodes-min 1 --nodes-max 1 --managed

Update kubeconfig:
aws eks --region us-east-1 update-kubeconfig --name project-cluster

Create deployment.yaml and service.yaml for Flask app.

Apply deployment: kubectl apply -f deployment.yaml

Verify pods & service: kubectl get pods, kubectl get svc

Access app via LoadBalancer external IP.

1️⃣3️⃣ Monitoring (Prometheus + Grafana)
Prometheus

Launch Ubuntu EC2 instance for Prometheus.

Download and configure Prometheus, set scrape target to Flask app external IP.

Run Prometheus:
/usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml

Verify Prometheus UI at port 9090.

Grafana

Launch Ubuntu EC2 instance for Grafana.

Install Grafana, start service.

Open Grafana UI at port 3000.

Add Prometheus as data source.

Build dashboards to monitor Flask app metrics.

1️⃣4️⃣ AWS Resource Cleanup (Optional)

Delete EKS cluster, deployments, services, S3/ECR resources if not needed.

Validate using AWS CLI and console.